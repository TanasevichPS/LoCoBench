#!/usr/bin/env python3
"""
–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è retrieval –º–æ–¥–µ–ª–∏ —Å Hugging Face
"""

import asyncio
import json
import logging
from pathlib import Path
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def run_full_retrieval_pipeline():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å retrieval"""
    print("="*60)
    print("üöÄ –ü–û–õ–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù RETRIEVAL –° HUGGING FACE")
    print("="*60)
    
    try:
        from locobench.core.config import Config
        from locobench.retrieval import retrieve_relevant, load_context_files_from_scenario
        from locobench.generation.synthetic_generator import MultiLLMGenerator
        from locobench.utils.llm_parsing import parse_llm_response
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = Config.from_yaml("config.yaml")
        
        # –í–∫–ª—é—á–∏—Ç—å retrieval
        config.retrieval.enabled = True
        config.retrieval.method = "embedding"
        config.retrieval.model_name = "all-MiniLM-L6-v2"
        config.retrieval.top_k = 5
        
        print("\n‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        print(f"   Retrieval: {'–í–∫–ª—é—á–µ–Ω' if config.retrieval.enabled else '–í—ã–∫–ª—é—á–µ–Ω'}")
        print(f"   –ú–µ—Ç–æ–¥: {config.retrieval.method}")
        print(f"   Embedding –º–æ–¥–µ–ª—å: {config.retrieval.model_name}")
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π
        scenario_file = Path("data/output/scenarios/test_hard_scenario.json")
        if not scenario_file.exists():
            print(f"‚ùå –°—Ü–µ–Ω–∞—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {scenario_file}")
            return
        
        with open(scenario_file, 'r') as f:
            scenario = json.load(f)
        
        print(f"\nüìã –°—Ü–µ–Ω–∞—Ä–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω: {scenario.get('id', 'unknown')}")
        print(f"   Difficulty: {scenario.get('difficulty', 'unknown')}")
        print(f"   Task: {scenario.get('title', 'unknown')}")
        
        # –ò–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã
        context_files = scenario.get('context_files', {})
        if isinstance(context_files, dict):
            context_files_dict = context_files
        else:
            context_files_dict = {}
        
        print(f"\nüìÅ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã: {len(context_files_dict)}")
        for filename in context_files_dict.keys():
            print(f"   - {filename}")
        
        # –®–∞–≥ 1: Retrieval
        print(f"\n{'='*60}")
        print("üîç –®–ê–ì 1: RETRIEVAL")
        print(f"{'='*60}")
        
        task_prompt = scenario.get('task_prompt', '')
        print(f"üìù –ó–∞–¥–∞—á–∞: {task_prompt[:100]}...")
        
        retrieved_context = retrieve_relevant(
            context_files_dict,
            task_prompt,
            top_k=config.retrieval.top_k,
            method=config.retrieval.method,
            model_name=config.retrieval.model_name
        )
        
        print(f"\n‚úÖ Retrieval –∑–∞–≤–µ—Ä—à–µ–Ω")
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(retrieved_context)} —Å–∏–º–≤–æ–ª–æ–≤")
        if retrieved_context:
            print(f"   –ü—Ä–µ–≤—å—é: {retrieved_context[:200]}...")
        else:
            print("   ‚ö†Ô∏è Retrieval –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        
        # –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å Hugging Face –º–æ–¥–µ–ª—å—é
        print(f"\n{'='*60}")
        print("ü§ñ –®–ê–ì 2: –ì–ï–ù–ï–†–ê–¶–ò–Ø –° HUGGING FACE")
        print(f"{'='*60}")
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞
        hf_model = "deepseek-ai/deepseek-coder-1.3b-instruct"
        print(f"   –ú–æ–¥–µ–ª—å: {hf_model}")
        
        # –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–º–ø—Ç —Å retrieval –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        full_prompt = f"""You are an expert Python developer. Your task is to provide a complete, working solution.

**TASK**: {scenario.get('title', 'Development Task')}

**DESCRIPTION**: {scenario.get('description', '')}

**REQUIREMENTS**: 
{task_prompt}

**RETRIEVED CONTEXT** (most relevant code fragments):
{retrieved_context if retrieved_context else 'No relevant context retrieved'}

**FULL CONTEXT FILES**: {', '.join(context_files_dict.keys())}

**CRITICAL INSTRUCTIONS**:
1. You MUST respond with valid JSON in the exact format shown below
2. Each file MUST contain complete, syntactically correct Python code
3. Use the retrieved context to understand the codebase structure
4. Do NOT truncate your response - provide the complete solution

**REQUIRED RESPONSE FORMAT**:
```json
{{
    "files": {{
        "pipeline.py": "def read_csv(filepath):\\n    import csv\\n    with open(filepath, 'r') as f:\\n        return list(csv.reader(f))"
    }}
}}
```

Generate your response now:"""
        
        print(f"\n‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—à–µ–Ω–∏—è (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏)...")
        
        generator = MultiLLMGenerator(config)
        
        response = await generator.generate_with_huggingface(hf_model, full_prompt)
        
        print(f"\n‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        print(f"   –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"\nüìù –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
        print(f"{response[:500]}...")
        
        # –®–∞–≥ 3: –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞
        print(f"\n{'='*60}")
        print("üîß –®–ê–ì 3: –ü–ê–†–°–ò–ù–ì –û–¢–í–ï–¢–ê")
        print(f"{'='*60}")
        
        parsed_files = parse_llm_response(response, expected_language='python')
        
        if parsed_files:
            print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω")
            print(f"   –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(parsed_files)}")
            for filename, content in parsed_files.items():
                print(f"\n   üìÑ {filename}:")
                print(f"   {'-'*40}")
                print(f"   {content[:300]}...")
                print(f"   –î–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        else:
            print(f"\n‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è")
            print(f"   –ü–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∫–æ–¥ –≤—Ä—É—á–Ω—É—é...")
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–π—Ç–∏ –∫–æ–¥ –≤ –±–ª–æ–∫–µ
            if "```python" in response:
                code_start = response.find("```python") + 9
                code_end = response.find("```", code_start)
                if code_end > code_start:
                    code = response[code_start:code_end].strip()
                    print(f"   –ù–∞–π–¥–µ–Ω –∫–æ–¥ –≤ –±–ª–æ–∫–µ: {len(code)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\n{'='*60}")
        print("üíæ –®–ê–ì 4: –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print(f"{'='*60}")
        
        results_dir = Path("evaluation_results")
        results_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"retrieval_pipeline_results_{timestamp}.json"
        
        results = {
            'timestamp': timestamp,
            'scenario_id': scenario.get('id', 'unknown'),
            'model': hf_model,
            'retrieval': {
                'enabled': True,
                'method': config.retrieval.method,
                'embedding_model': config.retrieval.model_name,
                'top_k': config.retrieval.top_k,
                'retrieved_context_length': len(retrieved_context),
                'retrieved_context_preview': retrieved_context[:500] if retrieved_context else None
            },
            'generation': {
                'response_length': len(response),
                'response_preview': response[:500]
            },
            'parsing': {
                'success': parsed_files is not None,
                'files_count': len(parsed_files) if parsed_files else 0,
                'parsed_files': parsed_files if parsed_files else None
            }
        }
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'='*60}")
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"‚úÖ Retrieval: {'–£—Å–ø–µ—à–Ω–æ' if retrieved_context else '–ù–µ —É–¥–∞–ª–æ—Å—å'}")
        print(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –£—Å–ø–µ—à–Ω–æ ({len(response)} —Å–∏–º–≤–æ–ª–æ–≤)")
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥: {'–£—Å–ø–µ—à–Ω–æ' if parsed_files else '–ù–µ —É–¥–∞–ª–æ—Å—å'}")
        if parsed_files:
            print(f"   –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(parsed_files)}")
            total_code = sum(len(c) for c in parsed_files.values())
            print(f"   –í—Å–µ–≥–æ –∫–æ–¥–∞: {total_code} —Å–∏–º–≤–æ–ª–æ–≤")
        
        print(f"\nüéâ –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
        return results
        
    except ImportError as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        print(f"   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install transformers torch sentence-transformers")
        return None
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}")
        import traceback
        print(traceback.format_exc())
        return None


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    results = await run_full_retrieval_pipeline()
    
    if results:
        print(f"\n‚úÖ –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ evaluation_results/")
    else:
        print(f"\n‚ùå –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")


if __name__ == "__main__":
    asyncio.run(main())
