#!/usr/bin/env python3
"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ DeepSeek —Å retrieval –∏ –±–µ–∑ –Ω–µ–≥–æ
"""

import asyncio
import json
import logging
from pathlib import Path
from datetime import datetime
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from locobench.core.config import Config
from locobench.evaluation.evaluator import Evaluator, run_evaluation

console = Console()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def run_evaluation_with_config(config_path: str, retrieval_enabled: bool, output_suffix: str):
    """–ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
    console.print(Panel.fit(f"üß™ –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ {'–°' if retrieval_enabled else '–ë–ï–ó'} retrieval", 
                           style="bold blue" if retrieval_enabled else "bold yellow"))
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = Config.from_yaml(config_path)
    
    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å retrieval
    config.retrieval.enabled = retrieval_enabled
    if retrieval_enabled:
        config.retrieval.method = "embedding"
        config.retrieval.model_name = "all-MiniLM-L6-v2"
        config.retrieval.top_k = 5
        config.retrieval.difficulties = ["hard", "expert"]
        console.print(f"‚úÖ Retrieval –≤–∫–ª—é—á–µ–Ω: –º–µ—Ç–æ–¥={config.retrieval.method}, –º–æ–¥–µ–ª—å={config.retrieval.model_name}")
    else:
        console.print("‚ùå Retrieval –æ—Ç–∫–ª—é—á–µ–Ω")
    
    # –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"
    
    # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–π—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏
    scenarios_dir = Path(config.data.output_dir) / "scenarios"
    
    if not scenarios_dir.exists():
        console.print(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {scenarios_dir}")
        console.print("üí° –°–æ–∑–¥–∞—é —Ç–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π...")
        
        # –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π
        scenarios_dir.mkdir(parents=True, exist_ok=True)
        test_scenario = create_test_scenario()
        scenario_file = scenarios_dir / "test_scenario.json"
        with open(scenario_file, 'w') as f:
            json.dump(test_scenario, f, indent=2)
        console.print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π: {scenario_file}")
    else:
        # –ù–∞–π—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏
        scenario_files = list(scenarios_dir.glob("*.json"))
        if not scenario_files:
            console.print(f"‚ö†Ô∏è –°—Ü–µ–Ω–∞—Ä–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {scenarios_dir}")
            console.print("üí° –°–æ–∑–¥–∞—é —Ç–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π...")
            test_scenario = create_test_scenario()
            scenario_file = scenarios_dir / "test_scenario.json"
            with open(scenario_file, 'w') as f:
                json.dump(test_scenario, f, indent=2)
            console.print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π: {scenario_file}")
        else:
            console.print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(scenario_files)} —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤")
            scenario_file = scenario_files[0]  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ü–µ–Ω–∞—Ä–∏–∏
    scenarios = []
    if scenario_file.exists():
        with open(scenario_file, 'r') as f:
            scenario_data = json.load(f)
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç
            if isinstance(scenario_data, dict):
                if 'scenarios' in scenario_data:
                    scenarios = scenario_data['scenarios']
                else:
                    scenarios = [scenario_data]  # –û–¥–∏–Ω —Å—Ü–µ–Ω–∞—Ä–∏–π
            elif isinstance(scenario_data, list):
                scenarios = scenario_data
    
    if not scenarios:
        console.print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ü–µ–Ω–∞—Ä–∏–∏")
        return None
    
    console.print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(scenarios)} —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤")
    
    # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 2-3)
    test_scenarios = scenarios[:2] if len(scenarios) >= 2 else scenarios
    console.print(f"üéØ –¢–µ—Å—Ç–∏—Ä—É—é –Ω–∞ {len(test_scenarios)} —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö")
    
    # –°–æ–∑–¥–∞—Ç—å evaluator
    evaluator = Evaluator(config, model_name=model_name)
    
    # –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ—Ü–µ–Ω–∫—É
    try:
        console.print(f"\nü§ñ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
        console.print(f"üìä –°—Ü–µ–Ω–∞—Ä–∏–µ–≤: {len(test_scenarios)}")
        
        results = await evaluator.evaluate_models(
            model_names=[model_name],
            scenarios=test_scenarios,
            resume=False  # –ù–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        )
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        output_dir = Path("evaluation_results")
        output_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"deepseek_evaluation_{output_suffix}_{timestamp}.json"
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        results_dict = {}
        for model_name_key, model_results in results.items():
            results_dict[model_name_key] = [
                {
                    'model_name': r.model_name,
                    'scenario_id': r.scenario_id,
                    'total_score': r.total_score,
                    'parsing_success': r.parsing_success,
                    'generation_time': r.generation_time,
                    'software_engineering_score': r.software_engineering_score,
                    'functional_correctness_score': r.functional_correctness_score,
                    'code_quality_score': r.code_quality_score,
                    'longcontext_utilization_score': r.longcontext_utilization_score,
                }
                for r in model_results
            ]
        
        with open(output_file, 'w') as f:
            json.dump({
                'retrieval_enabled': retrieval_enabled,
                'model': model_name,
                'scenarios_count': len(test_scenarios),
                'results': results_dict,
                'timestamp': timestamp
            }, f, indent=2)
        
        console.print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")
        
        # –°–æ–∑–¥–∞—Ç—å summary
        summaries = evaluator.generate_evaluation_summary(results)
        
        return {
            'results': results,
            'summaries': summaries,
            'retrieval_enabled': retrieval_enabled,
            'output_file': output_file
        }
        
    except Exception as e:
        console.print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ: {e}", style="bold red")
        import traceback
        console.print(traceback.format_exc())
        return None


def create_test_scenario():
    """–°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏"""
    return {
        "id": "test_python_easy_001",
        "title": "Calculate Factorial Function",
        "description": "Implement a factorial function in Python",
        "difficulty": "easy",
        "task_category": "feature_implementation",
        "language": "python",
        "task_prompt": "Write a Python function `factorial(n)` that calculates the factorial of a number n. The function should handle edge cases (n=0, n=1) and use recursion.",
        "context_files": {
            "utils.py": """
def calculate_sum(a, b):
    '''Calculate sum of two numbers'''
    return a + b

def calculate_product(a, b):
    '''Calculate product of two numbers'''
    return a * b
"""
        },
        "metadata": {
            "project_path": "./data/generated/test_project",
            "context_length": 500
        }
    }


async def compare_results():
    """–°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å retrieval –∏ –±–µ–∑ –Ω–µ–≥–æ"""
    console.print(Panel.fit("üî¨ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ DeepSeek —Å retrieval –∏ –±–µ–∑ –Ω–µ–≥–æ", style="bold cyan"))
    
    config_path = "config.yaml"
    
    # –ó–∞–ø—É—Å–∫ 1: –ë–ï–ó retrieval
    console.print("\n" + "="*60)
    results_without = await run_evaluation_with_config(config_path, retrieval_enabled=False, output_suffix="no_retrieval")
    
    console.print("\n" + "="*60)
    console.print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –≤—Ç–æ—Ä—ã–º –∑–∞–ø—É—Å–∫–æ–º...")
    await asyncio.sleep(2)
    
    # –ó–∞–ø—É—Å–∫ 2: –° retrieval
    console.print("\n" + "="*60)
    results_with = await run_evaluation_with_config(config_path, retrieval_enabled=True, output_suffix="with_retrieval")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    console.print("\n" + "="*60)
    console.print(Panel.fit("üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", style="bold green"))
    
    if results_without and results_with:
        # –ò–∑–≤–ª–µ—á—å summaries
        summary_without = results_without['summaries']
        summary_with = results_with['summaries']
        
        # –°–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_table = Table(title="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        comparison_table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="bold")
        comparison_table.add_column("–ë–ï–ó retrieval", style="yellow")
        comparison_table.add_column("–° retrieval", style="green")
        comparison_table.add_column("–†–∞–∑–Ω–∏—Ü–∞", style="cyan")
        
        model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"
        
        if model_name in summary_without and model_name in summary_with:
            s_without = summary_without[model_name]
            s_with = summary_with[model_name]
            
            # –û–±—â–∏–π —Å—á–µ—Ç
            comparison_table.add_row(
                "–û–±—â–∏–π —Å—á–µ—Ç (LCBS)",
                f"{s_without.avg_total_score:.3f}",
                f"{s_with.avg_total_score:.3f}",
                f"{s_with.avg_total_score - s_without.avg_total_score:+.3f}"
            )
            
            # Software Engineering
            comparison_table.add_row(
                "Software Engineering",
                f"{s_without.avg_software_engineering_score:.3f}",
                f"{s_with.avg_software_engineering_score:.3f}",
                f"{s_with.avg_software_engineering_score - s_without.avg_software_engineering_score:+.3f}"
            )
            
            # Functional Correctness
            comparison_table.add_row(
                "Functional Correctness",
                f"{s_without.avg_functional_correctness_score:.3f}",
                f"{s_with.avg_functional_correctness_score:.3f}",
                f"{s_with.avg_functional_correctness_score - s_without.avg_functional_correctness_score:+.3f}"
            )
            
            # Code Quality
            comparison_table.add_row(
                "Code Quality",
                f"{s_without.avg_code_quality_score:.3f}",
                f"{s_with.avg_code_quality_score:.3f}",
                f"{s_with.avg_code_quality_score - s_without.avg_code_quality_score:+.3f}"
            )
            
            # Long-Context Utilization
            comparison_table.add_row(
                "Long-Context Utilization",
                f"{s_without.avg_longcontext_utilization_score:.3f}",
                f"{s_with.avg_longcontext_utilization_score:.3f}",
                f"{s_with.avg_longcontext_utilization_score - s_without.avg_longcontext_utilization_score:+.3f}"
            )
            
            # Parsing Success Rate
            comparison_table.add_row(
                "Parsing Success Rate",
                f"{s_without.parsing_success_rate:.1%}",
                f"{s_with.parsing_success_rate:.1%}",
                f"{s_with.parsing_success_rate - s_without.parsing_success_rate:+.1%}"
            )
            
            # Average Generation Time
            comparison_table.add_row(
                "Avg Generation Time (s)",
                f"{s_without.avg_generation_time:.2f}",
                f"{s_with.avg_generation_time:.2f}",
                f"{s_with.avg_generation_time - s_without.avg_generation_time:+.2f}"
            )
            
            # Completed Scenarios
            comparison_table.add_row(
                "Completed Scenarios",
                f"{s_without.completed_scenarios}/{s_without.total_scenarios}",
                f"{s_with.completed_scenarios}/{s_with.total_scenarios}",
                f"{s_with.completed_scenarios - s_without.completed_scenarios:+d}"
            )
            
            console.print(comparison_table)
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
            comparison_file = Path("evaluation_results") / "comparison_summary.json"
            with open(comparison_file, 'w') as f:
                json.dump({
                    'model': model_name,
                    'timestamp': datetime.now().isoformat(),
                    'without_retrieval': {
                        'avg_total_score': s_without.avg_total_score,
                        'avg_software_engineering': s_without.avg_software_engineering_score,
                        'avg_functional_correctness': s_without.avg_functional_correctness_score,
                        'avg_code_quality': s_without.avg_code_quality_score,
                        'avg_longcontext_utilization': s_without.avg_longcontext_utilization_score,
                        'parsing_success_rate': s_without.parsing_success_rate,
                        'avg_generation_time': s_without.avg_generation_time,
                        'completed_scenarios': s_without.completed_scenarios,
                        'total_scenarios': s_without.total_scenarios
                    },
                    'with_retrieval': {
                        'avg_total_score': s_with.avg_total_score,
                        'avg_software_engineering': s_with.avg_software_engineering_score,
                        'avg_functional_correctness': s_with.avg_functional_correctness_score,
                        'avg_code_quality': s_with.avg_code_quality_score,
                        'avg_longcontext_utilization': s_with.avg_longcontext_utilization_score,
                        'parsing_success_rate': s_with.parsing_success_rate,
                        'avg_generation_time': s_with.avg_generation_time,
                        'completed_scenarios': s_with.completed_scenarios,
                        'total_scenarios': s_with.total_scenarios
                    },
                    'differences': {
                        'total_score_diff': s_with.avg_total_score - s_without.avg_total_score,
                        'parsing_success_diff': s_with.parsing_success_rate - s_without.parsing_success_rate,
                        'generation_time_diff': s_with.avg_generation_time - s_without.avg_generation_time
                    }
                }, f, indent=2)
            
            console.print(f"\n‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {comparison_file}")
            
        else:
            console.print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    else:
        console.print("‚ö†Ô∏è –û–¥–∏–Ω –∏–ª–∏ –æ–±–∞ –∑–∞–ø—É—Å–∫–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —Å –æ—à–∏–±–∫–æ–π")
        if not results_without:
            console.print("‚ùå –ó–∞–ø—É—Å–∫ –ë–ï–ó retrieval –Ω–µ —É–¥–∞–ª—Å—è")
        if not results_with:
            console.print("‚ùå –ó–∞–ø—É—Å–∫ –° retrieval –Ω–µ —É–¥–∞–ª—Å—è")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    await compare_results()


if __name__ == "__main__":
    asyncio.run(main())
