#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å MCP
"""

import asyncio
from pathlib import Path
from locobench.mcp_retrieval import retrieve_with_mcp

# –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
SAMPLE_CONTEXT_FILES = {
    "src/auth.py": """
def authenticate_user(username, password):
    # Simple authentication without input validation
    if username == "admin" and password == "admin123":
        return True
    return False
""",
    "src/security.py": """
import hashlib

def hash_password(password):
    return hashlib.md5(password.encode()).hexdigest()

def validate_input(user_input):
    # Basic validation
    if len(user_input) > 100:
        return False
    return True
""",
    "src/api.py": """
from flask import Flask, request

app = Flask(__name__)

@app.route('/login', methods=['POST'])
def login():
    username = request.form.get('username')
    password = request.form.get('password')
    # No input sanitization!
    return authenticate_user(username, password)
""",
}


def test_ollama():
    """–¢–µ—Å—Ç —Å Ollama"""
    print("=" * 60)
    print("–¢–µ—Å—Ç: MCP —Å Ollama")
    print("=" * 60)
    
    try:
        result = retrieve_with_mcp(
            context_files=SAMPLE_CONTEXT_FILES,
            task_prompt="–ù–∞–π—Ç–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞",
            task_category="security_analysis",
            project_dir=Path("."),
            provider="ollama",
            model="llama3.2",
            base_url="http://localhost:11434",
            use_llm=True,
        )
        
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:\n{result}\n")
        return result
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: ollama serve")
        print("üí° –ò –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: ollama pull llama3.2")
        return None


def test_huggingface():
    """–¢–µ—Å—Ç —Å Hugging Face"""
    print("=" * 60)
    print("–¢–µ—Å—Ç: MCP —Å Hugging Face")
    print("=" * 60)
    
    try:
        result = retrieve_with_mcp(
            context_files=SAMPLE_CONTEXT_FILES,
            task_prompt="–ù–∞–π—Ç–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞",
            task_category="security_analysis",
            project_dir=Path("."),
            provider="huggingface",
            model="meta-llama/Llama-3.2-3B-Instruct",
            use_llm=True,
        )
        
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:\n{result}\n")
        return result
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install transformers torch")
        print("üí° –ò —É –≤–∞—Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏")
        return None


def test_local_openai():
    """–¢–µ—Å—Ç —Å LocalAI/LM Studio"""
    print("=" * 60)
    print("–¢–µ—Å—Ç: MCP —Å LocalAI/LM Studio")
    print("=" * 60)
    
    try:
        result = retrieve_with_mcp(
            context_files=SAMPLE_CONTEXT_FILES,
            task_prompt="–ù–∞–π—Ç–∏ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞",
            task_category="security_analysis",
            project_dir=Path("."),
            provider="local_openai",
            model="llama-3.2",
            base_url="http://localhost:1234",  # LM Studio default
            use_llm=True,
        )
        
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:\n{result}\n")
        return result
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ LM Studio –∑–∞–ø—É—â–µ–Ω –∏ —Å–µ—Ä–≤–µ—Ä –∞–∫—Ç–∏–≤–µ–Ω")
        print("üí° –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ LocalAI –Ω–∞ http://localhost:1234")
        return None


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("\n" + "=" * 60)
    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å MCP")
    print("=" * 60 + "\n")
    
    print("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä:")
    print("1. Ollama (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)")
    print("2. Hugging Face")
    print("3. LocalAI/LM Studio")
    print("4. –í—Å–µ")
    
    choice = input("\n–í–∞—à –≤—ã–±–æ—Ä (1-4): ").strip()
    
    if choice == "1":
        test_ollama()
    elif choice == "2":
        test_huggingface()
    elif choice == "3":
        test_local_openai()
    elif choice == "4":
        test_ollama()
        print()
        test_huggingface()
        print()
        test_local_openai()
    else:
        print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ó–∞–ø—É—Å–∫–∞—é —Ç–µ—Å—Ç Ollama –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é...")
        test_ollama()
    
    print("=" * 60)
    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    print("=" * 60)


if __name__ == "__main__":
    main()
