"""
MCP-based retrieval system for LoCoBench.

This module provides intelligent file retrieval using Model Context Protocol (MCP) tools.
LLM can dynamically choose which files are needed for a specific task type.
"""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Set
from enum import Enum

logger = logging.getLogger(__name__)


class TaskCategory(Enum):
    """Task categories supported by MCP retrieval"""
    SECURITY_ANALYSIS = "security_analysis"
    ARCHITECTURAL_UNDERSTANDING = "architectural_understanding"
    CODE_COMPREHENSION = "code_comprehension"
    FEATURE_IMPLEMENTATION = "feature_implementation"
    BUG_INVESTIGATION = "bug_investigation"
    CROSS_FILE_REFACTORING = "cross_file_refactoring"
    INTEGRATION_TESTING = "integration_testing"
    MULTI_SESSION_DEVELOPMENT = "multi_session_development"


class MCPTool:
    """Represents an MCP tool for file retrieval"""
    
    def __init__(
        self,
        name: str,
        description: str,
        parameters: Dict[str, str],
        handler: callable,
    ):
        self.name = name
        self.description = description
        self.parameters = parameters
        self.handler = handler
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert tool to dictionary format for LLM"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    param: {"type": "string", "description": desc}
                    for param, desc in self.parameters.items()
                },
                "required": list(self.parameters.keys()),
            },
        }
    
    def execute(self, **kwargs) -> List[Dict[str, Any]]:
        """Execute the tool handler"""
        return self.handler(**kwargs)


class LoCoBenchMCPServer:
    """
    MCP Server for intelligent file retrieval in LoCoBench.
    
    Provides specialized tools for different task categories to help LLM
    intelligently select relevant files for each task type.
    """
    
    def __init__(
        self,
        project_dir: Path,
        task_category: str,
        context_files: Dict[str, str],
        task_prompt: str,
    ):
        self.project_dir = project_dir
        self.task_category = task_category
        self.context_files = context_files
        self.task_prompt = task_prompt
        self.selected_files: Set[str] = set()
        self.tools = self._initialize_tools()
    
    def _initialize_tools(self) -> List[MCPTool]:
        """Initialize tools based on task category"""
        category_tools_map = {
            'security_analysis': self._get_security_tools,
            'architectural_understanding': self._get_architectural_tools,
            'code_comprehension': self._get_comprehension_tools,
            'feature_implementation': self._get_implementation_tools,
            'bug_investigation': self._get_bug_investigation_tools,
            'cross_file_refactoring': self._get_refactoring_tools,
            'integration_testing': self._get_integration_testing_tools,
            'multi_session_development': self._get_multi_session_tools,
        }
        
        category_lower = self.task_category.lower()
        for key, tools_func in category_tools_map.items():
            if key in category_lower:
                return tools_func()  # Call the function to get tools
        
        # Default tools
        return self._get_default_tools()
    
    # ==================== Security Analysis Tools ====================
    
    def _get_security_tools(self) -> List[MCPTool]:
        """Tools for security analysis tasks"""
        return [
            MCPTool(
                name="find_security_sensitive_files",
                description=(
                    "–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é: "
                    "–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è, –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è, –≤–∞–ª–∏–¥–∞—Ü–∏—è, —à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ, "
                    "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ä–æ–ª–µ–π, —Ç–æ–∫–µ–Ω–æ–≤, —Å–µ—Å—Å–∏–π"
                ),
                parameters={
                    "keywords": "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ (security, auth, encrypt, validate, etc.)",
                    "file_patterns": "–ü–∞—Ç—Ç–µ—Ä–Ω—ã –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ (auth*.py, security*.py, etc.)",
                },
                handler=self._find_security_sensitive_files,
            ),
            MCPTool(
                name="analyze_dependency_graph_for_security",
                description=(
                    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π. "
                    "–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≤–≤–æ–¥ –∏–ª–∏ "
                    "–≤—ã–ø–æ–ª–Ω—è—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏"
                ),
                parameters={
                    "entry_points": "–¢–æ—á–∫–∏ –≤—Ö–æ–¥–∞ –≤ —Å–∏—Å—Ç–µ–º—É (API endpoints, handlers)",
                    "sensitive_operations": "–û–ø–µ—Ä–∞—Ü–∏–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
                },
                handler=self._analyze_security_dependencies,
            ),
            MCPTool(
                name="find_input_validation_points",
                description=(
                    "–ù–∞—Ö–æ–¥–∏—Ç –º–µ—Å—Ç–∞, –≥–¥–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞. "
                    "–í–∞–∂–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π —Ç–∏–ø–∞ injection"
                ),
                parameters={
                    "input_sources": "–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –≤–≤–æ–¥–∞ (API, forms, files, etc.)",
                },
                handler=self._find_input_validation_points,
            ),
        ]
    
    def _find_security_sensitive_files(
        self,
        keywords: str = "",
        file_patterns: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Find files related to security"""
        security_keywords = [
            'security', 'auth', 'encrypt', 'validate', 'sanitize',
            'vulnerability', 'exploit', 'attack', 'password', 'token',
            'permission', 'access', 'authorization', 'authentication',
            'session', 'csrf', 'xss', 'sql injection', 'input validation'
        ]
        
        if keywords:
            security_keywords.extend(keywords.split(','))
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_lower = content.lower()
            file_lower = file_path.lower()
            
            # Check keywords in content or filename
            if any(keyword in content_lower or keyword in file_lower 
                   for keyword in security_keywords):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.9,  # High relevance for security
                    "reason": "Contains security-related code",
                })
        
        return found_files
    
    def _analyze_security_dependencies(
        self,
        entry_points: str = "",
        sensitive_operations: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Analyze dependency graph for security concerns"""
        # This would analyze imports and call graphs
        # For now, return files that import security-related modules
        security_modules = ['hashlib', 'secrets', 'cryptography', 'jwt', 'bcrypt']
        
        found_files = []
        for file_path, content in self.context_files.items():
            if any(f"import {mod}" in content or f"from {mod}" in content 
                   for mod in security_modules):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.85,
                    "reason": "Uses security libraries",
                })
        
        return found_files
    
    def _find_input_validation_points(
        self,
        input_sources: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Find input validation points"""
        validation_patterns = [
            'validate', 'sanitize', 'clean', 'escape', 'filter',
            'input', 'request', 'form', 'parameter', 'query'
        ]
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_lower = content.lower()
            if any(pattern in content_lower for pattern in validation_patterns):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.8,
                    "reason": "Contains input validation logic",
                })
        
        return found_files
    
    # ==================== Architectural Understanding Tools ====================
    
    def _get_architectural_tools(self) -> List[MCPTool]:
        """Tools for architectural understanding tasks"""
        return [
            MCPTool(
                name="identify_core_components",
                description=(
                    "–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã: "
                    "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã, –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏, –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, "
                    "–æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥—É–ª–∏ –∏ –∏—Ö —Å–≤—è–∑–∏"
                ),
                parameters={
                    "component_types": "–¢–∏–ø—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ (interface, abstract, pattern)",
                },
                handler=self._identify_core_components,
            ),
            MCPTool(
                name="map_dependency_hierarchy",
                description=(
                    "–°—Ç—Ä–æ–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏. "
                    "–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞"
                ),
                parameters={
                    "root_components": "–ö–æ—Ä–Ω–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
                    "max_depth": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π",
                },
                handler=self._map_dependency_hierarchy,
            ),
            MCPTool(
                name="find_design_patterns",
                description=(
                    "–ù–∞—Ö–æ–¥–∏—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –∫–æ–¥–µ: "
                    "Factory, Builder, Singleton, Strategy, Observer –∏ –¥—Ä."
                ),
                parameters={
                    "pattern_types": "–¢–∏–ø—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞",
                },
                handler=self._find_design_patterns,
            ),
        ]
    
    def _identify_core_components(
        self,
        component_types: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Identify core architectural components"""
        architectural_keywords = [
            'interface', 'abstract', 'pattern', 'component', 'module',
            'structure', 'design', 'architecture', 'framework', 'blueprint',
            'schema', 'layout', 'hierarchy', 'composition', 'decomposition'
        ]
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_lower = content.lower()
            file_lower = file_path.lower()
            
            # Check for architectural indicators
            if any(keyword in content_lower or keyword in file_lower 
                   for keyword in architectural_keywords):
                # Also check for common patterns
                if ('class' in content and ('interface' in content_lower or 
                    'abstract' in content_lower or 'base' in content_lower)):
                    found_files.append({
                        "path": file_path,
                        "content": content,
                        "relevance_score": 0.9,
                        "reason": "Core architectural component",
                    })
        
        return found_files
    
    def _map_dependency_hierarchy(
        self,
        root_components: str = "",
        max_depth: str = "3",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Map dependency hierarchy"""
        # Find files with many imports (likely core components)
        found_files = []
        for file_path, content in self.context_files.items():
            import_count = content.count('import ') + content.count('from ')
            if import_count > 5:  # Files with many dependencies
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.85,
                    "reason": "High dependency count - likely core component",
                })
        
        return found_files
    
    def _find_design_patterns(
        self,
        pattern_types: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Find design patterns in code"""
        pattern_keywords = [
            'factory', 'builder', 'singleton', 'strategy', 'observer',
            'adapter', 'decorator', 'facade', 'proxy', 'command'
        ]
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_lower = content.lower()
            if any(pattern in content_lower for pattern in pattern_keywords):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.8,
                    "reason": "Contains design pattern",
                })
        
        return found_files
    
    # ==================== Code Comprehension Tools ====================
    
    def _get_comprehension_tools(self) -> List[MCPTool]:
        """Tools for code comprehension tasks"""
        return [
            MCPTool(
                name="trace_execution_flow",
                description=(
                    "–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –ø–æ—Ç–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –æ—Ç —Ç–æ—á–∫–∏ –≤—Ö–æ–¥–∞. "
                    "–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã –≤ –ø–æ—Ä—è–¥–∫–µ –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–π"
                ),
                parameters={
                    "entry_point": "–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ (main function, API endpoint, etc.)",
                    "target_function": "–¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è",
                },
                handler=self._trace_execution_flow,
            ),
            MCPTool(
                name="find_related_functions",
                description=(
                    "–ù–∞—Ö–æ–¥–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π "
                    "—á–µ—Ä–µ–∑ –≤—ã–∑–æ–≤—ã, –∏–º–ø–æ—Ä—Ç—ã –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ"
                ),
                parameters={
                    "function_name": "–ò–º—è —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–≤—è–∑–µ–π",
                },
                handler=self._find_related_functions,
            ),
            MCPTool(
                name="analyze_data_flow",
                description=(
                    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É. "
                    "–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –¥–∞–Ω–Ω—ã–µ"
                ),
                parameters={
                    "data_sources": "–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö",
                    "data_sinks": "–ü—Ä–∏–µ–º–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö",
                },
                handler=self._analyze_data_flow,
            ),
        ]
    
    def _trace_execution_flow(
        self,
        entry_point: str = "",
        target_function: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Trace execution flow"""
        # Find files with main functions or entry points
        found_files = []
        for file_path, content in self.context_files.items():
            if 'if __name__' in content or 'def main' in content.lower():
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.9,
                    "reason": "Entry point",
                })
        
        return found_files
    
    def _find_related_functions(
        self,
        function_name: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Find functions related to target function"""
        found_files = []
        for file_path, content in self.context_files.items():
            if function_name and function_name in content:
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.85,
                    "reason": f"References function: {function_name}",
                })
        
        return found_files
    
    def _analyze_data_flow(
        self,
        data_sources: str = "",
        data_sinks: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Analyze data flow"""
        data_keywords = ['data', 'process', 'transform', 'convert', 'parse']
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in data_keywords):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.8,
                    "reason": "Handles data flow",
                })
        
        return found_files
    
    # ==================== Feature Implementation Tools ====================
    
    def _get_implementation_tools(self) -> List[MCPTool]:
        """Tools for feature implementation tasks"""
        return [
            MCPTool(
                name="find_implementation_examples",
                description=(
                    "–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∫ –ø—Ä–∏–º–µ—Ä. "
                    "–ò—â–µ—Ç —Ñ–∞–π–ª—ã —Å –ø–æ—Ö–æ–∂–µ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é"
                ),
                parameters={
                    "feature_type": "–¢–∏–ø —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤",
                    "similar_features": "–ü–æ—Ö–æ–∂–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –ø—Ä–æ–µ–∫—Ç–µ",
                },
                handler=self._find_implementation_examples,
            ),
            MCPTool(
                name="identify_integration_points",
                description=(
                    "–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ—á–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–ª—è –Ω–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏: "
                    "API endpoints, service interfaces, configuration files"
                ),
                parameters={
                    "feature_requirements": "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ñ—É–Ω–∫—Ü–∏–∏",
                },
                handler=self._identify_integration_points,
            ),
            MCPTool(
                name="find_related_configurations",
                description=(
                    "–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ñ—É–Ω–∫—Ü–∏–µ–π: "
                    "settings, config, constants, environment variables"
                ),
                parameters={
                    "feature_domain": "–î–æ–º–µ–Ω —Ñ—É–Ω–∫—Ü–∏–∏",
                },
                handler=self._find_related_configurations,
            ),
        ]
    
    def _find_implementation_examples(
        self,
        feature_type: str = "",
        similar_features: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Find similar implementation examples"""
        # This would use semantic similarity to find similar code
        # For now, return files with similar structure
        found_files = []
        for file_path, content in self.context_files.items():
            if 'def ' in content and 'class ' in content:
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.8,
                    "reason": "Contains implementation examples",
                })
        
        return found_files
    
    def _identify_integration_points(
        self,
        feature_requirements: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Identify integration points"""
        integration_keywords = ['api', 'endpoint', 'interface', 'service', 'handler']
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in integration_keywords):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.85,
                    "reason": "Integration point",
                })
        
        return found_files
    
    def _find_related_configurations(
        self,
        feature_domain: str = "",
        **kwargs,  # –ü—Ä–∏–Ω—è—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> List[Dict[str, Any]]:
        """Find related configuration files"""
        config_patterns = ['config', 'settings', 'constants', 'env', 'yaml', 'json']
        
        found_files = []
        for file_path, content in self.context_files.items():
            file_lower = file_path.lower()
            if any(pattern in file_lower for pattern in config_patterns):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.8,
                    "reason": "Configuration file",
                })
        
        return found_files
    
    # ==================== Bug Investigation Tools ====================
    
    def _get_bug_investigation_tools(self) -> List[MCPTool]:
        """Tools for bug investigation tasks"""
        return [
            MCPTool(
                name="trace_error_path",
                description=(
                    "–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –ø—É—Ç—å –æ—à–∏–±–∫–∏ –æ—Ç –º–µ—Å—Ç–∞ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –¥–æ —Ç–æ—á–∫–∏ –≤—Ö–æ–¥–∞. "
                    "–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã –≤ —Å—Ç–µ–∫–µ –≤—ã–∑–æ–≤–æ–≤"
                ),
                parameters={
                    "error_message": "–°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ",
                    "error_location": "–ú–µ—Å—Ç–æ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏—è –æ—à–∏–±–∫–∏",
                },
                handler=self._trace_error_path,
            ),
            MCPTool(
                name="find_error_handlers",
                description=(
                    "–ù–∞—Ö–æ–¥–∏—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –æ—à–∏–±–æ–∫, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–±–ª–µ–º–æ–π: "
                    "try-except –±–ª–æ–∫–∏, error handlers, logging"
                ),
                parameters={
                    "error_type": "–¢–∏–ø –æ—à–∏–±–∫–∏",
                },
                handler=self._find_error_handlers,
            ),
            MCPTool(
                name="analyze_test_coverage",
                description=(
                    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏ –ø—Ä–æ–±–ª–µ–º–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. "
                    "–ù–∞—Ö–æ–¥–∏—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–±–ª–µ–º–Ω—ã–º –∫–æ–¥–æ–º"
                ),
                parameters={
                    "problem_area": "–ü—Ä–æ–±–ª–µ–º–Ω–∞—è –æ–±–ª–∞—Å—Ç—å",
                },
                handler=self._analyze_test_coverage,
            ),
        ]
    
    def _trace_error_path(
        self,
        error_message: str = "",
        error_location: str = "",
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """Trace error path"""
        error_keywords = ['error', 'exception', 'fail', 'raise', 'catch']
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in error_keywords):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.85,
                    "reason": "Error handling code",
                })
        
        return found_files
    
    def _find_error_handlers(
        self,
        error_type: str = "",
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """Find error handlers"""
        handler_patterns = ['try:', 'except', 'catch', 'error_handler', 'on_error']
        
        found_files = []
        for file_path, content in self.context_files.items():
            if any(pattern in content for pattern in handler_patterns):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.8,
                    "reason": "Error handler",
                })
        
        return found_files
    
    def _analyze_test_coverage(
        self,
        problem_area: str = "",
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """Analyze test coverage"""
        test_patterns = ['test_', '_test', 'spec', 'specification']
        
        found_files = []
        for file_path, content in self.context_files.items():
            file_lower = file_path.lower()
            if any(pattern in file_lower for pattern in test_patterns):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.75,
                    "reason": "Test file",
                })
        
        return found_files
    
    # ==================== Refactoring Tools ====================
    
    def _get_refactoring_tools(self) -> List[MCPTool]:
        """Tools for cross-file refactoring tasks"""
        return [
            MCPTool(
                name="identify_refactoring_targets",
                description=(
                    "–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏—Ç—å: "
                    "–¥—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è –∫–æ–¥, –Ω–∞—Ä—É—à–µ–Ω–∏–µ DRY, code smells"
                ),
                parameters={
                    "refactoring_goal": "–¶–µ–ª—å —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞",
                },
                handler=self._identify_refactoring_targets,
            ),
            MCPTool(
                name="map_cross_file_dependencies",
                description=(
                    "–°—Ç—Ä–æ–∏—Ç –∫–∞—Ä—Ç—É –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏ –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞. "
                    "–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤–º–µ—Å—Ç–µ"
                ),
                parameters={
                    "target_files": "–¶–µ–ª–µ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞",
                },
                handler=self._map_cross_file_dependencies,
            ),
        ]
    
    def _identify_refactoring_targets(
        self,
        refactoring_goal: str = "",
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """Identify refactoring targets"""
        # Find files with many similar patterns (potential duplication)
        found_files = []
        for file_path, content in self.context_files.items():
            # Simple heuristic: files with many similar function definitions
            if content.count('def ') > 10:
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.8,
                    "reason": "Potential refactoring target",
                })
        
        return found_files
    
    def _map_cross_file_dependencies(
        self,
        target_files: str = "",
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """Map cross-file dependencies"""
        # Find files that import many other files (likely refactoring candidates)
        found_files = []
        for file_path, content in self.context_files.items():
            import_count = content.count('import ') + content.count('from ')
            if import_count > 8:
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.85,
                    "reason": "High cross-file dependencies",
                })
        
        return found_files
    
    # ==================== Integration Testing Tools ====================
    
    def _get_integration_testing_tools(self) -> List[MCPTool]:
        """Tools for integration testing tasks"""
        return [
            MCPTool(
                name="find_integration_points",
                description="–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏",
                parameters={
                    "components": "–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏",
                },
                handler=self._find_integration_points,
            ),
        ]
    
    def _find_integration_points(
        self,
        components: str = "",
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """Find integration points"""
        integration_keywords = ['integration', 'integrate', 'connect', 'bridge', 'adapter']
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in integration_keywords):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.85,
                    "reason": "Integration point",
                })
        
        return found_files
    
    # ==================== Multi-Session Development Tools ====================
    
    def _get_multi_session_tools(self) -> List[MCPTool]:
        """Tools for multi-session development tasks"""
        return [
            MCPTool(
                name="find_state_management",
                description="–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã, —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏",
                parameters={
                    "state_type": "–¢–∏–ø —Å–æ—Å—Ç–æ—è–Ω–∏—è",
                },
                handler=self._find_state_management,
            ),
        ]
    
    def _find_state_management(
        self,
        state_type: str = "",
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """Find state management files"""
        state_keywords = ['state', 'session', 'cache', 'store', 'persist', 'memory']
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in state_keywords):
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": 0.85,
                    "reason": "State management",
                })
        
        return found_files
    
    # ==================== Default Tools ====================
    
    def _get_default_tools(self) -> List[MCPTool]:
        """Default tools for unknown task categories"""
        return [
            MCPTool(
                name="find_relevant_files",
                description="–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–∞–¥–∞—á–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤",
                parameters={
                    "keywords": "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–¥–∞—á–∏",
                },
                handler=self._find_relevant_files_default,
            ),
        ]
    
    def _find_relevant_files_default(
        self,
        keywords: str = "",
        **kwargs,
    ) -> List[Dict[str, Any]]:
        """Default file finder"""
        # Extract keywords from task prompt
        task_words = set(self.task_prompt.lower().split())
        
        found_files = []
        for file_path, content in self.context_files.items():
            content_words = set(content.lower().split())
            overlap = len(task_words & content_words)
            
            if overlap > 5:  # At least 5 common words
                found_files.append({
                    "path": file_path,
                    "content": content,
                    "relevance_score": min(overlap / 20, 0.9),
                    "reason": f"Keyword overlap: {overlap} words",
                })
        
        return found_files
    
    # ==================== Main Retrieval Method ====================
    
    def get_tools_for_llm(self) -> List[Dict[str, Any]]:
        """Get tools in format suitable for LLM"""
        return [tool.to_dict() for tool in self.tools]
    
    def execute_tool_call(self, tool_name: str, parameters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Execute a tool call and return results"""
        for tool in self.tools:
            if tool.name == tool_name:
                try:
                    results = tool.execute(**parameters)
                    # Track selected files
                    for result in results:
                        self.selected_files.add(result["path"])
                    return results
                except Exception as e:
                    logger.error(f"Error executing tool {tool_name}: {e}")
                    return []
        
        logger.warning(f"Tool {tool_name} not found")
        return []
    
    def format_selected_context(self) -> str:
        """Format selected files into context string"""
        if not self.selected_files:
            return ""
        
        parts = []
        for file_path in sorted(self.selected_files):
            if file_path in self.context_files:
                parts.append(f"### {file_path}\n```\n{self.context_files[file_path]}\n```")
        
        return "\n\n".join(parts)


def retrieve_with_mcp(
    context_files: Dict[str, str],
    task_prompt: str,
    task_category: str,
    project_dir: Path,
    config=None,  # Config object for LLM clients
    provider: str = "openai",
    model: Optional[str] = None,
    use_llm: bool = True,
    base_url: Optional[str] = None,  # For local providers (Ollama, LocalAI)
    api_key: Optional[str] = None,  # For LocalAI
) -> str:
    """
    Synchronous wrapper for retrieve_with_mcp_async.
    
    Main entry point for MCP-based retrieval.
    
    Args:
        context_files: Available context files
        task_prompt: Task description
        task_category: Category of the task
        project_dir: Project directory
        config: Configuration object (optional, for LLM clients)
        provider: LLM provider:
            - Cloud: "openai", "anthropic"
            - Local: "ollama", "huggingface" (or "hf"), "local_openai" (or "local")
        model: Model name (optional, uses default from config or provider)
        use_llm: Whether to use LLM for tool calling (default: True)
        base_url: Base URL for local providers (default: http://localhost:11434 for Ollama)
        api_key: API key for LocalAI (optional)
    
    Returns:
        Formatted context string with selected files
    """
    import asyncio
    
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If loop is already running, we need to use a different approach
            import concurrent.futures
            
            def run_in_thread():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(
                        retrieve_with_mcp_async(
                            context_files=context_files,
                            task_prompt=task_prompt,
                            task_category=task_category,
                            project_dir=project_dir,
                            config=config,
                            provider=provider,
                            model=model,
                            use_llm=use_llm,
                            base_url=base_url,
                            api_key=api_key,
                        )
                    )
                finally:
                    new_loop.close()
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_in_thread)
                return future.result(timeout=300)  # 5 minute timeout
        else:
            return loop.run_until_complete(
                retrieve_with_mcp_async(
                    context_files=context_files,
                    task_prompt=task_prompt,
                    task_category=task_category,
                    project_dir=project_dir,
                    config=config,
                    provider=provider,
                    model=model,
                    use_llm=use_llm,
                    base_url=base_url,
                    api_key=api_key,
                )
            )
    except RuntimeError:
        # No event loop, create a new one
        return asyncio.run(
            retrieve_with_mcp_async(
                context_files=context_files,
                task_prompt=task_prompt,
                task_category=task_category,
                project_dir=project_dir,
                config=config,
                provider=provider,
                model=model,
                use_llm=use_llm,
                base_url=base_url,
                api_key=api_key,
            )


async def retrieve_with_mcp_async(
    context_files: Dict[str, str],
    task_prompt: str,
    task_category: str,
    project_dir: Path,
    config=None,  # Config object for LLM clients
    provider: str = "openai",
    model: Optional[str] = None,
    use_llm: bool = True,
    base_url: Optional[str] = None,
    api_key: Optional[str] = None,
) -> str:
    """
    Async main entry point for MCP-based retrieval.
    
    This function integrates with an LLM to intelligently select files
    using MCP tools. If use_llm=False, falls back to simple heuristic-based selection.
    
    Args:
        context_files: Available context files
        task_prompt: Task description
        task_category: Category of the task
        project_dir: Project directory
        config: Configuration object (optional, for LLM clients)
        provider: LLM provider ("openai" or "anthropic")
        model: Model name (optional, uses default from config)
        use_llm: Whether to use LLM for tool calling (default: True)
    
    Returns:
        Formatted context string with selected files
    """
    server = LoCoBenchMCPServer(
        project_dir=project_dir,
        task_category=task_category,
        context_files=context_files,
        task_prompt=task_prompt,
    )
    
    if use_llm:
        # Use LLM for intelligent tool calling
        try:
            # Check if provider is a local model provider
            if provider in ("ollama", "huggingface", "local_openai", "hf", "local"):
                from .mcp_local_llm import retrieve_with_local_llm
                
                # Normalize provider name
                if provider in ("hf", "huggingface"):
                    provider = "huggingface"
                elif provider == "local":
                    provider = "local_openai"
                
                return await retrieve_with_local_llm(
                    context_files=context_files,
                    task_prompt=task_prompt,
                    task_category=task_category,
                    project_dir=project_dir,
                    provider=provider,
                    model=model or "llama3.2",  # Default model for local providers
                    base_url=base_url,
                    api_key=api_key,
                    config=config,
                )
            else:
                # Use cloud LLM (OpenAI/Anthropic)
                from .mcp_llm_integration import retrieve_with_mcp_llm
                
                return await retrieve_with_mcp_llm(
                    context_files=context_files,
                    task_prompt=task_prompt,
                    task_category=task_category,
                    project_dir=project_dir,
                    config=config,
                    provider=provider,
                    model=model,
                )
        except Exception as e:
            logger.warning(f"MCP LLM integration failed: {e}. Falling back to heuristic-based selection.")
            import traceback
            logger.debug(traceback.format_exc())
            # Fall through to heuristic-based selection
    
    # Fallback: Simple heuristic-based selection
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —Å —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º–∏
    try:
        from .mcp_heuristics import retrieve_with_mcp_heuristics
        
        logger.info("üîß Using MCP heuristics-based retrieval (no LLM)")
        return retrieve_with_mcp_heuristics(
            context_files=context_files,
            task_prompt=task_prompt,
            task_category=task_category,
            project_dir=project_dir,
        )
    except ImportError:
        # –ï—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –≤–µ—Ä—Å–∏—é
        logger.debug("MCP heuristics module not available, using basic fallback")
        
        if server.tools:
            # Execute all tools with basic parameters
            all_results = []
            for tool in server.tools:
                try:
                    # Extract keywords from task prompt
                    keywords = " ".join(set(task_prompt.lower().split()[:10]))
                    results = tool.execute(keywords=keywords)
                    all_results.extend(results)
                except Exception as e:
                    logger.debug(f"Tool {tool.name} failed: {e}")
            
            # Deduplicate by path
            seen_paths = set()
            unique_results = []
            for result in all_results:
                path = result.get("path", "")
                if path and path not in seen_paths:
                    seen_paths.add(path)
                    unique_results.append(result)
                    server.selected_files.add(path)
            
            # Format results
            return server.format_selected_context()
    
    return ""
