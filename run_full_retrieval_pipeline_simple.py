#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è retrieval –º–æ–¥–µ–ª–∏ —Å Hugging Face
–†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é –±–µ–∑ –ø–æ–ª–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LoCoBench
"""

import asyncio
import json
import logging
from pathlib import Path
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def run_simple_retrieval_pipeline():
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å retrieval –∏ Hugging Face"""
    print("="*60)
    print("üöÄ –ü–û–õ–ù–´–ô –ü–ê–ô–ü–õ–ê–ô–ù RETRIEVAL –° HUGGING FACE")
    print("   (–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)")
    print("="*60)
    
    try:
        # –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥—É–ª–µ–π
        from sentence_transformers import SentenceTransformer
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch
        import numpy as np
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ü–µ–Ω–∞—Ä–∏–π
        scenario_file = Path("data/output/scenarios/test_hard_scenario.json")
        if not scenario_file.exists():
            print(f"‚ùå –°—Ü–µ–Ω–∞—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {scenario_file}")
            return None
        
        with open(scenario_file, 'r') as f:
            scenario = json.load(f)
        
        print(f"\n‚úÖ –°—Ü–µ–Ω–∞—Ä–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω: {scenario.get('id', 'unknown')}")
        print(f"   Difficulty: {scenario.get('difficulty', 'unknown')}")
        print(f"   Task: {scenario.get('title', 'unknown')}")
        
        context_files = scenario.get('context_files', {})
        task_prompt = scenario.get('task_prompt', '')
        
        print(f"\nüìÅ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã: {len(context_files)}")
        
        # –®–ê–ì 1: RETRIEVAL
        print(f"\n{'='*60}")
        print("üîç –®–ê–ì 1: RETRIEVAL")
        print(f"{'='*60}")
        
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ embedding –º–æ–¥–µ–ª–∏...")
        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("‚úÖ Embedding –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –†–∞–∑–¥–µ–ª–∏—Ç—å –∫–æ–¥ –Ω–∞ —á–∞–Ω–∫–∏
        def split_code(code, chunk_size=512):
            lines = code.split('\n')
            chunks = []
            current_chunk = []
            current_size = 0
            
            for line in lines:
                line_size = len(line) + 1
                if current_size + line_size > chunk_size and current_chunk:
                    chunks.append('\n'.join(current_chunk))
                    current_chunk = [line]
                    current_size = line_size
                else:
                    current_chunk.append(line)
                    current_size += line_size
            
            if current_chunk:
                chunks.append('\n'.join(current_chunk))
            
            return chunks
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —á–∞–Ω–∫–∏
        all_chunks = []
        chunk_info = []
        
        for filename, code_content in context_files.items():
            file_chunks = split_code(code_content)
            for idx, chunk in enumerate(file_chunks):
                chunk_info.append((filename, idx, chunk))
                all_chunks.append(chunk)
        
        print(f"   –°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(context_files)} —Ñ–∞–π–ª–æ–≤")
        
        # –í—ã—á–∏—Å–ª–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        print("   –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        all_texts = all_chunks + [task_prompt]
        embeddings = embedding_model.encode(all_texts, show_progress_bar=False)
        
        query_embedding = embeddings[-1]
        chunk_embeddings = embeddings[:-1]
        
        # –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        query_norm = np.linalg.norm(query_embedding)
        chunk_norms = np.linalg.norm(chunk_embeddings, axis=1)
        similarities = np.dot(chunk_embeddings, query_embedding) / (chunk_norms * query_norm)
        similarities = np.nan_to_num(similarities, nan=0.0)
        
        # –ü–æ–ª—É—á–∏—Ç—å top-K
        top_k = 5
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        # –°–æ–±—Ä–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        retrieved_parts = []
        for idx in top_indices:
            file_path, chunk_idx, chunk_content = chunk_info[idx]
            similarity_score = similarities[idx]
            retrieved_parts.append(
                f"From {file_path} (chunk {chunk_idx + 1}, similarity: {similarity_score:.3f}):\n{chunk_content}"
            )
        
        retrieved_context = "\n\n".join(retrieved_parts)
        
        print(f"‚úÖ Retrieval –∑–∞–≤–µ—Ä—à–µ–Ω")
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(retrieved_context)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   Top-K: {top_k}")
        if retrieved_context:
            print(f"   –ü—Ä–µ–≤—å—é: {retrieved_context[:200]}...")
        
        # –®–ê–ì 2: –ì–ï–ù–ï–†–ê–¶–ò–Ø –° HUGGING FACE
        print(f"\n{'='*60}")
        print("ü§ñ –®–ê–ì 2: –ì–ï–ù–ï–†–ê–¶–ò–Ø –° HUGGING FACE")
        print(f"{'='*60}")
        
        hf_model = "deepseek-ai/deepseek-coder-1.3b-instruct"
        print(f"   –ú–æ–¥–µ–ª—å: {hf_model}")
        print(f"   ‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)...")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
        tokenizer = AutoTokenizer.from_pretrained(hf_model, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            hf_model,
            trust_remote_code=True,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            low_cpu_mem_usage=True
        )
        
        if device == "cpu":
            model = model.to(device)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–º–ø—Ç
        full_prompt = f"""You are an expert Python developer. Provide a complete solution.

**TASK**: {scenario.get('title', 'Development Task')}

**DESCRIPTION**: {scenario.get('description', '')}

**REQUIREMENTS**: 
{task_prompt}

**RETRIEVED CONTEXT** (most relevant code fragments):
{retrieved_context if retrieved_context else 'No relevant context retrieved'}

**FULL CONTEXT FILES**: {', '.join(context_files.keys())}

**INSTRUCTIONS**:
1. Respond with valid JSON format
2. Provide complete Python code
3. Use the retrieved context to understand the codebase structure

**REQUIRED FORMAT**:
```json
{{
    "files": {{
        "pipeline.py": "def read_csv(filepath):\\n    import csv\\n    with open(filepath, 'r') as f:\\n        return list(csv.reader(f))"
    }}
}}
```

Generate your response:"""
        
        print(f"\n   –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(full_prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –ò–∑–≤–ª–µ—á—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å
        if full_prompt in generated_text:
            response = generated_text.split(full_prompt, 1)[1].strip()
        else:
            response = generated_text[len(full_prompt):].strip()
        
        print(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        print(f"   –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(response)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"\nüìù –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:")
        print(f"{response}")
        
        # –®–ê–ì 3: –ü–ê–†–°–ò–ù–ì
        print(f"\n{'='*60}")
        print("üîß –®–ê–ì 3: –ü–ê–†–°–ò–ù–ì –û–¢–í–ï–¢–ê")
        print(f"{'='*60}")
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ JSON
        parsed_files = None
        
        # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–π—Ç–∏ JSON –≤ –æ—Ç–≤–µ—Ç–µ
        import re
        json_patterns = [
            r'```json\s*\n?(.*?)\n?\s*```',
            r'```\s*\n?(.*?)\n?\s*```',
            r'(\{.*?"files".*?\})',
        ]
        
        for pattern in json_patterns:
            matches = re.findall(pattern, response, re.DOTALL)
            for match in matches:
                try:
                    data = json.loads(match)
                    if 'files' in data and isinstance(data['files'], dict):
                        parsed_files = data['files']
                        print(f"‚úÖ JSON –Ω–∞–π–¥–µ–Ω –∏ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω")
                        break
                except:
                    continue
        
        if not parsed_files:
            # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–π—Ç–∏ –∫–æ–¥ –Ω–∞–ø—Ä—è–º—É—é
            if "```python" in response:
                code_start = response.find("```python") + 9
                code_end = response.find("```", code_start)
                if code_end > code_start:
                    code = response[code_start:code_end].strip()
                    parsed_files = {"extracted.py": code}
                    print(f"‚úÖ –ö–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ –±–ª–æ–∫–∞")
        
        if parsed_files:
            print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω")
            print(f"   –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(parsed_files)}")
            for filename, content in parsed_files.items():
                print(f"\n   üìÑ {filename}:")
                print(f"   {'-'*40}")
                print(f"   {content[:400]}...")
                print(f"   –î–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        else:
            print(f"\n‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è")
            print(f"   –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # –®–ê–ì 4: –°–û–•–†–ê–ù–ï–ù–ò–ï
        print(f"\n{'='*60}")
        print("üíæ –®–ê–ì 4: –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print(f"{'='*60}")
        
        results_dir = Path("evaluation_results")
        results_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = results_dir / f"retrieval_pipeline_results_{timestamp}.json"
        
        results = {
            'timestamp': timestamp,
            'scenario_id': scenario.get('id', 'unknown'),
            'model': hf_model,
            'device': device,
            'retrieval': {
                'enabled': True,
                'embedding_model': 'all-MiniLM-L6-v2',
                'top_k': top_k,
                'chunks_processed': len(all_chunks),
                'retrieved_context_length': len(retrieved_context),
                'retrieved_context': retrieved_context[:1000] if retrieved_context else None
            },
            'generation': {
                'response_length': len(response),
                'response': response
            },
            'parsing': {
                'success': parsed_files is not None,
                'files_count': len(parsed_files) if parsed_files else 0,
                'parsed_files': parsed_files if parsed_files else None
            }
        }
        
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
        
        # –ò–¢–û–ì–ò
        print(f"\n{'='*60}")
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print(f"{'='*60}")
        print(f"‚úÖ Retrieval: –£—Å–ø–µ—à–Ω–æ ({len(retrieved_context)} —Å–∏–º–≤–æ–ª–æ–≤)")
        print(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: –£—Å–ø–µ—à–Ω–æ ({len(response)} —Å–∏–º–≤–æ–ª–æ–≤)")
        print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥: {'–£—Å–ø–µ—à–Ω–æ' if parsed_files else '–ß–∞—Å—Ç–∏—á–Ω–æ'}")
        if parsed_files:
            total_code = sum(len(c) for c in parsed_files.values())
            print(f"   –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(parsed_files)}")
            print(f"   –í—Å–µ–≥–æ –∫–æ–¥–∞: {total_code} —Å–∏–º–≤–æ–ª–æ–≤")
        
        print(f"\nüéâ –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÑ –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results_file}")
        
        return results
        
    except ImportError as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        print(f"   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch sentence-transformers numpy")
        return None
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        print(traceback.format_exc())
        return None


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    results = await run_simple_retrieval_pipeline()
    
    if results:
        print(f"\n‚úÖ –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    else:
        print(f"\n‚ùå –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")


if __name__ == "__main__":
    asyncio.run(main())
